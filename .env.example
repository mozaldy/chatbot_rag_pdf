# --- Application Settings ---
API_TITLE="Local RAG API"

# --- Qdrant Configuration ---
QDRANT_HOST="localhost"
QDRANT_PORT=6333
QDRANT_TIMEOUT=8
COLLECTION_NAME="pdf_rag"
DOCUMENT_LIST_MAX_POINTS=10000
DOCUMENT_LIST_PAGE_SIZE=256

# --- Embedding Models ---
# Dense Model (Standard: BAAI/bge-m3)
EMBEDDING_MODEL="BAAI/bge-m3"

# --- Chunking Configuration ---
# Chunk size in tokens (larger = more context, smaller = more precise)
# Recommended: 512 for technical docs, 256-384 for Q&A
CHUNK_SIZE=512
# Overlap between chunks (prevents information loss at boundaries)
# Recommended: 20-30% of chunk size
CHUNK_OVERLAP=128

# --- PDF Processing ---
# Set to True if you need to read text inside images/scans
ENABLE_OCR=False

# --- Retrieval Configuration ---
# Controls balance between keyword matching (0.0) and semantic matching (1.0)
# Lower = better for exact text queries, Higher = better for conceptual queries
HYBRID_ALPHA=0.3
DENSE_TOP_K=24
SPARSE_TOP_K=24
FUSION_MODE="weighted_rrf"
FUSION_RRF_K=60
RERANKER_TYPE="cross_encoder"
RERANKER_MODEL="BAAI/bge-reranker-base"
RERANKER_USE_FP16=True
RERANKER_BATCH_SIZE=16
RERANKER_MAX_LENGTH=512
RERANK_CANDIDATES_K=24
RERANK_TOP_K=8
RERANK_MIN_SCORE=0.1
MAX_CONTEXT_CHARS_PER_CHUNK=1800
MAX_SOURCES=5
CONTEXT_DIVERSITY_ENABLED=True
MAX_SOURCES_PER_DOC=2
REQUIRE_VALID_CITATIONS=True
RETRIEVAL_DEBUG_LOGS=False

# Optional query rewrite before retrieval
QUERY_REWRITE_ENABLED=True
QUERY_REWRITE_MODE="rule"
QUERY_REWRITE_WEIGHT=0.6
QUERY_REWRITE_MAX_TERMS=12
QUERY_REWRITE_MAX_CHARS=200

# Conversation-aware retrieval
CONVERSATION_HISTORY_MAX_MESSAGES=8
CONVERSATION_STANDALONE_QUERY_ENABLED=True
CONVERSATION_STANDALONE_MAX_CHARS=280

# --- Indexing Throughput ---
UPSERT_BATCH_SIZE=20

# --- LLM Configuration (Main Inference) ---
# Options: "ollama", "openai", "anthropic", "gemini"
LLM_PROVIDER="ollama"
# Model examples: "gemma3:4b", "gpt-4o", "claude-3-opus", "gemini-2.0-flash"
LLM_MODEL="gemma3:4b"

# --- LLM Configuration (Fast/Enrichment) ---
# Used for document summarization and chunk enrichment
FAST_LLM_PROVIDER="ollama"
FAST_LLM_MODEL="gemma3:4b"

# --- Provider Specific Settings ---

# Ollama
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_TIMEOUT=120.0

# OpenAI
# OPENAI_API_KEY="sk-..."

# Anthropic
# ANTHROPIC_API_KEY="sk-ant-..."

# Google Gemini
# GOOGLE_API_KEY="AIza..."
