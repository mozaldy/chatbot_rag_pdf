# --- Application Settings ---
API_TITLE="Local RAG API"

# --- Qdrant Configuration ---
QDRANT_HOST="localhost"
QDRANT_PORT=6333
QDRANT_TIMEOUT=8
COLLECTION_NAME="pdf_rag"
DOCUMENT_LIST_MAX_POINTS=10000
DOCUMENT_LIST_PAGE_SIZE=256

# --- Embedding Models ---
# Dense Model (Standard: BAAI/bge-m3)
EMBEDDING_MODEL="BAAI/bge-m3"

# --- Chunking Configuration ---
# Chunk size in tokens (larger = more context, smaller = more precise)
# Recommended: 512 for technical docs, 256-384 for Q&A
CHUNK_SIZE=512
# Overlap between chunks (prevents information loss at boundaries)
# Recommended: 20-30% of chunk size
CHUNK_OVERLAP=128
CHUNKING_SCHEMA_VERSION=2
ENABLE_TABLE_ANCHORS=True
TABLE_ANCHOR_MAX_PER_TABLE=6

# --- PDF Processing ---
# Set to True if you need to read text inside images/scans
ENABLE_OCR=True
ENABLE_PICTURE_DESCRIPTION=True

# Docling picture description backend:
# - local: use local SmolVLM/Granite-style picture description models
# - gemini_api: call Gemini through OpenAI-compatible chat completions API
PICTURE_DESCRIPTION_PROVIDER="local"
PICTURE_DESCRIPTION_MODEL="gemini-2.0-flash"
PICTURE_DESCRIPTION_API_URL="https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
PICTURE_DESCRIPTION_TIMEOUT=30.0
PICTURE_DESCRIPTION_CONCURRENCY=1
PICTURE_DESCRIPTION_ONLY_CHARTS=True
PICTURE_DESCRIPTION_PROMPT="Describe this chart or figure for document retrieval. Include title, axes, legend, trend direction, and notable numeric values if visible."

# Table visual interpretation for SOP flow symbols embedded inside table grids.
# This runs Gemini vision on rendered table images and appends an interpretation
# block below each table in the ingested markdown.
ENABLE_TABLE_VISUAL_INTERPRETATION=True
TABLE_VISUAL_INTERPRETATION_PROVIDER="gemini_api"
TABLE_VISUAL_INTERPRETATION_MODEL="gemini-2.5-flash-lite"
TABLE_VISUAL_INTERPRETATION_API_URL="https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
TABLE_VISUAL_INTERPRETATION_TIMEOUT=45.0
TABLE_VISUAL_INTERPRETATION_CONCURRENCY=2
TABLE_VISUAL_INTERPRETATION_MAX_CHARS=1600
TABLE_VISUAL_ROUTING_MODE="auto_signal"
TABLE_VISUAL_DETECTOR_BIAS="precision"
TABLE_VISUAL_SIGNAL_THRESHOLD=0.75
TABLE_VISUAL_REQUIRE_STRONG_SIGNALS=True
TABLE_VISUAL_RETRY_ENABLED=True
TABLE_VISUAL_MAX_RETRIES=5
TABLE_VISUAL_BACKOFF_SECONDS=2.0
TABLE_VISUAL_BACKOFF_MULTIPLIER=2.0
TABLE_VISUAL_INTERPRETATION_PROMPT="You are reading a table image from a Standard Operating Procedure (SOP). Identify visual process flow semantics that are not captured by plain table cells, including arrows, connectors, decision symbols, and handoffs across roles. Return concise markdown bullets only. Do not hallucinate."

# NOTE: In current Docling versions, chart extraction uses a local Granite Vision
# chart model. Keep this False on low-VRAM GPUs if you hit CUDA OOM.
ENABLE_CHART_EXTRACTION=False

# --- Retrieval Configuration ---
# Controls balance between keyword matching (0.0) and semantic matching (1.0)
# Lower = better for exact text queries, Higher = better for conceptual queries
HYBRID_ALPHA=0.5
DENSE_TOP_K=24
SPARSE_TOP_K=24
FUSION_MODE="weighted_rrf"
FUSION_RRF_K=60
RERANKER_TYPE="cross_encoder"
RERANKER_MODEL="BAAI/bge-reranker-base"
RERANKER_USE_FP16=True
RERANKER_BATCH_SIZE=16
RERANKER_MAX_LENGTH=512
RERANK_CANDIDATES_K=24
RERANK_TOP_K=8
RERANK_MIN_SCORE=0.05
MAX_CONTEXT_CHARS_PER_CHUNK=1800
TABLE_PARENT_EXPANSION_ENABLED=True
TABLE_PARENT_ALWAYS_FULL_CONTEXT=True
MAX_SOURCES=5
CONTEXT_DIVERSITY_ENABLED=True
MAX_SOURCES_PER_DOC=2
REQUIRE_VALID_CITATIONS=True
RETRIEVAL_DEBUG_LOGS=False

# Optional query rewrite before retrieval
QUERY_REWRITE_ENABLED=True
QUERY_REWRITE_MODE="rule"
QUERY_REWRITE_WEIGHT=0.6
QUERY_REWRITE_MAX_TERMS=12
QUERY_REWRITE_MAX_CHARS=200

# Conversation-aware retrieval
CONVERSATION_HISTORY_MAX_MESSAGES=8
CONVERSATION_STANDALONE_QUERY_ENABLED=True
CONVERSATION_STANDALONE_MAX_CHARS=280

# --- Indexing Throughput ---
UPSERT_BATCH_SIZE=20

# --- LLM Configuration (Main Inference) ---
# Options: "ollama", "openai", "anthropic", "gemini"
LLM_PROVIDER="gemini"
# Model examples: "gemma3:4b", "gpt-4o", "claude-3-opus", "gemini-2.0-flash"
LLM_MODEL="gemini-2.5-flash"

# --- LLM Configuration (Fast/Enrichment) ---
# Used for document summarization and chunk enrichment
FAST_LLM_PROVIDER="gemini"
FAST_LLM_MODEL="gemini-2.5-flash-lite"

# --- Provider Specific Settings ---

# Ollama
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_TIMEOUT=120.0

# OpenAI
# OPENAI_API_KEY="sk-..."

# Anthropic
# ANTHROPIC_API_KEY="sk-ant-..."

# Google Gemini
# GOOGLE_API_KEY="AIza..."
